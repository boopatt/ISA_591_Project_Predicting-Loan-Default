---
title: "Project5"
author: "Group 5AA_Black-Thenmozhi Boopathy"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)

if(require(pacman)==0)
   {install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
```
# OOS PERFORMANCE COMPETITION - Predict New Observations as Default = Yes or No

In this project, we are using the given scoring data that contains 79243 observations with 36 variables. We are doing the pre processing methods for the  scoreing data.For the initial start, just looking at the structure to make a proper execution in predictor variables.


```{r}

df = read.csv("score.csv", stringsAsFactors = TRUE)
train_df = read.csv("train.csv", stringsAsFactors = TRUE)
train_median <- median(train_df$revol_util, na.rm = TRUE)
str(df)

```
# Summary Statistics for numeric Variables 

We make sure which variables names are comes under the category of bank_numeric and bank_factor using head(bank_numeric) head(bank_factor).

```{r}
#Split data into categorical and numeric datasets 
bank_numeric <- select_if(df, is.numeric)
bank_factor <- select_if(df, is.factor)
#Ensure dataframes correctly split
head(bank_numeric)
head(bank_factor)
```
# Summary Statistics for numeric Variables

Next, we are trying to understand the patterns and detect anomalies and simplify the dataset for targeting  better prediction. Each variable gives important values such as the mean and median with the min and max values helping identify outliers. We can also find early indicators of numeric variables that could potentially be manipulated into factor variables or factor lumped to help our prediction model.

```{r}
summary(bank_numeric)
```
# Plot for Missing Values

In this stage, we are focusing on the missing values to decide which predictors need imputation strategy or dropping the variables if its less than 10%.

mths_since_last_delinq has 53.17%. 
revol_util = 56 missing values
mort_acc = 7485 missing values
pub__rec_bankruptcies = 104 missing values
mths_since_last_delinq = 42134 missing values

```{r}
plot_missing(df)
```

# Look at structure and ensure these are all factor variables

```{r}
str(bank_factor)

```
# Data visualization

# Histograms for numeric variables

These variables can easily be seen for their skew, symmetry, and range. If the range is large from the models, like pub_rec, revol_bal, dti, and annual_inc, this indicates outliers in the data.

```{r}

DataExplorer::plot_histogram(bank_numeric,
                             ggtheme=theme_minimal(),
                             ncol=3,
                             nrow=3)

```
# Create a heat map to check correlation between variables

Based on these results, we can confirm outliers in the data and deal with them accordingly. Also, we see that these numeric variables can have correlation to one defaulting on a loan. Lets look at a heat map to finalize some of the cleaning techniques we will perform

```{r}
DataExplorer::plot_correlation(bank_numeric,
                               type="continuous",
                               cor_args = list("use" = "pairwise.complete.obs")
                               )


```
Total_acc and open_acc has correlated metrics with an R^2 of .68, while pub_rec and pub_rec_bankrupcies have correlated features with an R^2 of 0.68. Installment has an extremely high correlation to loan amount, showing we can remove this variable.

# Variables we are Removing

acc_now_delinq: We are removing this variable because most of the values are zero and it wouldn’t give us any special indication of loan default.
mths_since_last_delinq: We will remove this variable because there are too many null values(53.22%).Most people probably don’t have any delinquency, so this variable can be disregarded.
delinq_2yrs: Most of the values are zero. but we can also just get rid of it.
installment: We are removing this variable because of its high correlation (0.95) to loan amount, meaning they provide the same information.

```{r}
# Remove these variables
df<-df%>%
  select(-installment, -acc_now_delinq, -mths_since_last_delinq, -delinq_2yrs)
dim(df)
```

We now have only 32 variables in our original data frame. (36-4=32). None of these variables show major distribution between the ‘yes’ and ’no decision of a loan default based on the boxplot and amount of null values.

#  Cateogrical Variables:

Now, we will look at the factor variables and decide where to make changes based on the prediction power and levels. We will also recode and lump variables together if there are too many levels to the data or remove when there’s too many levels in itself.

# Look at structure and levels for categorical variables
```{r}
DataExplorer::plot_bar(bank_factor,
                       ggtheme = theme_minimal(),
                       ncol=4,
                       nrow=5)
                       
```
 There are 12 variables shown here. Notice how some variables are not included in the visual, such as emp_title, issue_d, earliest_cr_line, address, and last_credit_pull_d. These variables all have a significant amount of levels, meaning they could either be manipulated to a numeric variable or be removed because there would way too many dummy variables. Even some of the bar plots have too many levels that will need to be analyzed further later in our analysis.

Now, we will identify what we are doing with each variable. For the categorical variables we are keeping, we will specify changing the leels or leaving the data as it is.

# Keep the Same

Term: 2 levels, we will keep the same
Grade: 7 levels, keep the same
Verification_status: 2 levels, Keep the same
Initial_list_status: 2 levels, Keep the same
Debt_settlement_flag: a “yes” decision has a high correlation to “yes” to loan_default, keep the same

# Variables we are Removing

Emp_title: Remove (too many levels)
Issue_d: Remove (No significance to loan_default_
Title: Remove (Too many levels)
Application_type: Remove (Vast majority of values in “individual” value)
Hardship_flag: Remove (only 1 level)
Earliest_cr_line: Dates do not seem significant in predicting loan default
Last_credit_pull_d: Dates do not seem significant in predicting loan default
Address (zip code): Discriminatory variable, not significant in explaining loan default
sub_grade: Has the same properties as grade, not needed in our model

```{r}
# Remove variables that are not seen as needed
df<-df%>%
  select(-emp_title, -issue_d, -title, -application_type, -hardship_flag, -earliest_cr_line, -last_credit_pull_d, -address, -sub_grade)
dim(df)
```

# Handling Missing Data

Earlier, we identified some of the missing values in different variables. Although we removed the mnths_since_last_delinq variable, we still have three variables that are missing data:


```{r}
#Checking missing Values
colSums(is.na(df))
```
# Outlier/Anomaly Detection

Throughout our data cleaning process, we have identified numerous variables with outliers. Below, we will decide what to do with these variables. In deciding what to do, we must ask ourselves: Are these outliers realistic in context of the real world?

```{r}
#DEC
df$fico_score <- (df$fico_range_low + df$fico_range_high) / 2
df <- df %>% select(-fico_range_low, -fico_range_high)

#cap <- quantile(df$annual_inc, 0.99)
#df$annual_inc[df$annual_inc > cap] <- cap


# Median imputation on the mort_acc variable

df$mort_acc[is.na(df$mort_acc)] <- 1
#df$mort_acc[is.na(df$mort_acc)] <- median(df$mort_acc, na.rm = TRUE)
sum(is.na(df$mort_acc))

```
```{r}
#Median imputation to the pub_rec_bankrupcies variable
df$pub_rec_bankruptcies[is.na(df$pub_rec_bankruptcies)] <- 0
sum(is.na(df$pub_rec_bankruptcies))
```
Here, we are making the strategy of using median imputation in revol_util for making the na's are effective in prediction. Because we need all the observations in predictions of loan_default.

```{r}
# Removing rows where revol_util is NA 
#df <- df[!is.na(df$revol_util), ]
df$revol_util[is.na(df$revol_util)] <- train_median
#nrow(df)
```
```{r}
# Capping the revol_util variable
Q1 <- quantile(df$revol_util, 0.25, na.rm = TRUE)
Q3 <- quantile(df$revol_util, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$revol_util[df$revol_util > upper_bound] <- upper_bound
summary(df$revol_util)
boxplot(df$revol_util, main = "Revolving Utilization", horizontal = TRUE)
```
```{r}
# Capping the dti outlier
Q1 <- quantile(df$dti, 0.25, na.rm = TRUE)
Q3 <- quantile(df$dti, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

upper_bound <- Q3 + 1.5 * IQR
df$dti[df$dti > upper_bound] <- upper_bound
boxplot(df$dti, main = "DTI", horizontal = TRUE)
```
```{r}
#Removing the "other" and "none" values 
#df <- df[!df$home_ownership %in% c("OTHER", "NONE"), ]
df$home_ownership[df$home_ownership %in% c("OTHER","NONE")] <- "RENT"
df$home_ownership <- droplevels(df$home_ownership)
table(df$home_ownership)
```
```{r}
#Reduce dimensions in the purpose variable
df$purpose <- fct_other(df$purpose, keep = c("debt_consolidation", "credit_card", "home_improvement"), other_level = "other")
table(df$purpose)
```

```{r}
# Reducing dimensions on emp_length
df <- df %>%
  mutate(emp_length = case_when(
    emp_length %in% c("< 1 year", "1 year", "2 years", "3 years") ~ "0-3 years",
    emp_length %in% c("4 years", "5 years", "6 years") ~ "4-6 years",
    emp_length %in% c("7 years", "8 years", "9 years") ~ "7-9 years",
    emp_length == "10+ years" ~ "10+ years",
    TRUE ~ "Unknown"
  ))
df$emp_length <- factor(df$emp_length,
                                levels = c("0-3 years", "4-6 years", "7-9 years", "10+ years","Unknown"),
                                ordered = TRUE)
#df <- df %>%
#  filter(!is.na(emp_length) & emp_length != "Unknown")

```

```{r}
# Dummy encoding all remaining categorical variables 
df <- fastDummies::dummy_cols(df,
                              select_columns = c(
                                                 "term",
                                                 "home_ownership",
                                                 "verification_status",
                                                 "grade",
                                                 "purpose",
                                                 "initial_list_status",
                                                 "debt_settlement_flag",
                                                 "emp_length"),
                              remove_first_dummy = TRUE,      # avoids dummy variable trap
                              remove_selected_columns = TRUE) # removes original factor columns
```
```{r}
# dti * int_rate interaction term 
df$interaction_dti_interest <- df$dti * df$int_rate
# Show correlation
hist(df$interaction_dti_interest,
     main = "Interaction",
     xlab = "DTI * int_rate",
     col = "lightblue",
     border = "white")

```
# Conclusion: Summary of our Cleaned Data

After performing all of these steps to create the best data frame to train a model, we have resulted in this dataset:

```{r}
summary(df)
```
# Removing Extra Spaces in Predictor Variables and Save the File for Final Execution 

```{r}
names(df) <- gsub(" ", "_", names(df))
names(df) <- gsub("-", "_", names(df))  
names(df) <- gsub("\\+", "plus", names(df))
saveRDS(df, "score_df.rds")
```

# Evaluate the performance using Logit Model 

Here, We are using the logit model for evaluating the performance on loan_default. We already performed the construction on an model in our previous project. so we are going to call the model and doing the predictions with our cleaned scoring data and setting the prob thershold as greater than 0.5. our levels are 0 and 1. Also, we made the new dataframe contains only id and loan default for production environment in the form of csv file.

```{r}

logit_model = readRDS("dec2_xgb_model.rds")
# Predict using the trained model
#y_pred <- predict(logit_Model, newdata=df)
#y_pred <- ifelse(y_pred == "Yes", 1, 0)

pred.prob <- predict(logit_model, 
                           newdata = df, 
                           type = "prob")[,"Yes"]

y_pred <- factor(ifelse(pred.prob > 0.2246494,
                                      "1","0"),
                               levels=c("1","0"))



# Combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)

# Write to CSV
write.csv(predictions, "group5AA_Black-Boopathy_submission_file.csv", row.names = FALSE)
```
