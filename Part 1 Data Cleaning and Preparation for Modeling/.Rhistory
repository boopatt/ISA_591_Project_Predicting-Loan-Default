#ggtheme = theme_minimal(),
#ncol=3,
#nrow=3)
# Create a heat map to check correlation between variables
DataExplorer::plot_correlation(bank_numeric,
type="continuous",
cor_args = list("use" = "pairwise.complete.obs")
)
# Remove these variables
df<-df%>%
select(-installment, -acc_now_delinq, -mths_since_last_delinq, -delinq_2yrs)
dim(df)
DataExplorer::plot_bar(bank_factor,
ggtheme = theme_minimal(),
ncol=4,
nrow=5)
# Remove variables that are not seen as needed
df<-df%>%
select(-emp_title, -issue_d, -title, -application_type, -hardship_flag, -earliest_cr_line, -last_credit_pull_d, -address, -sub_grade)
dim(df)
#Checking missing Values
colSums(is.na(df))
#DEC
df$fico_score <- (df$fico_range_low + df$fico_range_high) / 2
df <- df %>% select(-fico_range_low, -fico_range_high)
#cap <- quantile(df$annual_inc, 0.99)
#df$annual_inc[df$annual_inc > cap] <- cap
# Median imputation on the mort_acc variable
df$mort_acc[is.na(df$mort_acc)] <- 1
#df$mort_acc[is.na(df$mort_acc)] <- median(df$mort_acc, na.rm = TRUE)
sum(is.na(df$mort_acc))
#Median imputation to the pub_rec_bankrupcies variable
df$pub_rec_bankruptcies[is.na(df$pub_rec_bankruptcies)] <- 0
sum(is.na(df$pub_rec_bankruptcies))
# Removing rows where revol_util is NA
#df <- df[!is.na(df$revol_util), ]
df$revol_util[is.na(df$revol_util)] <- train_median
#nrow(df)
# Capping the revol_util variable
Q1 <- quantile(df$revol_util, 0.25, na.rm = TRUE)
Q3 <- quantile(df$revol_util, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$revol_util[df$revol_util > upper_bound] <- upper_bound
summary(df$revol_util)
boxplot(df$revol_util, main = "Revolving Utilization", horizontal = TRUE)
# Capping the dti outlier
Q1 <- quantile(df$dti, 0.25, na.rm = TRUE)
Q3 <- quantile(df$dti, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$dti[df$dti > upper_bound] <- upper_bound
boxplot(df$dti, main = "DTI", horizontal = TRUE)
#Removing the "other" and "none" values
#df <- df[!df$home_ownership %in% c("OTHER", "NONE"), ]
df$home_ownership[df$home_ownership %in% c("OTHER","NONE")] <- "RENT"
df$home_ownership <- droplevels(df$home_ownership)
table(df$home_ownership)
#Reduce dimensions in the purpose variable
df$purpose <- fct_other(df$purpose, keep = c("debt_consolidation", "credit_card", "home_improvement"), other_level = "other")
table(df$purpose)
# Reducing dimensions on emp_length
df <- df %>%
mutate(emp_length = case_when(
emp_length %in% c("< 1 year", "1 year", "2 years", "3 years") ~ "0-3 years",
emp_length %in% c("4 years", "5 years", "6 years") ~ "4-6 years",
emp_length %in% c("7 years", "8 years", "9 years") ~ "7-9 years",
emp_length == "10+ years" ~ "10+ years",
TRUE ~ "Unknown"
))
df$emp_length <- factor(df$emp_length,
levels = c("0-3 years", "4-6 years", "7-9 years", "10+ years","Unknown"),
ordered = TRUE)
#df <- df %>%
#  filter(!is.na(emp_length) & emp_length != "Unknown")
# Dummy encoding all remaining categorical variables
df <- fastDummies::dummy_cols(df,
select_columns = c(
"term",
"home_ownership",
"verification_status",
"grade",
"purpose",
"initial_list_status",
"debt_settlement_flag",
"emp_length"),
remove_first_dummy = TRUE,      # avoids dummy variable trap
remove_selected_columns = TRUE) # removes original factor columns
# dti * int_rate interaction term
df$interaction_dti_interest <- df$dti * df$int_rate
# Show correlation
hist(df$interaction_dti_interest,
main = "Interaction",
xlab = "DTI * int_rate",
col = "lightblue",
border = "white")
summary(df)
names(df) <- gsub(" ", "_", names(df))
names(df) <- gsub("-", "_", names(df))
names(df) <- gsub("\\+", "plus", names(df))
saveRDS(df, "score_df.rds")
xgb_model = readRDS("dec2_logit_model.rds")
# Predict using the trained model
#y_pred <- predict(xgb_Model, newdata=df)
#y_pred <- ifelse(y_pred == "Yes", 1, 0)
pred.prob <- predict(xgb_model,
newdata = df,
type = "prob")[,"Yes"]
y_pred <- factor(ifelse(pred.prob > 0.5288394,
"1","0"),
levels=c("1","0"))
# Combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)
# Write to CSV
write.csv(predictions, "group5AA_Black-Boopathy_submission_file.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)
if(require(pacman)==0)
{install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes,dslabs,scatterplot3d,rpart,rpart.plot,randomForest,xgboost,ROCR)
if (!require(mlba)) {
library(devtools)
install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
df=readRDS("dec2_train_df.rds")
holdout=readRDS("dec2_holdout_df.rds")
levels(df$loan_default)
df$loan_default<-relevel(df$loan_default, ref= "Yes")
levels(df$loan_default)
# Downsample majority to match minority
set.seed(123)
df$loan_default <- as.factor(df$loan_default)
df.us=downSample(x=df %>% dplyr::select(-loan_default),
y=df$loan_default,
yname="loan_default"
)
table(df$loan_default)
table(df.us$loan_default)
ctrl <- trainControl(method = "repeatedcv", number = 10,
repeats=3,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = 'final')
tuneGrid <- expand.grid(
alpha = seq(0, 1, by = 0.1),
#alpha = seq(0.1, 1, by = 0.2),         # Elastic Net mixing
#lambda = seq(0.001, 0.1, length = 10)    # Regularization strength
lambda = 10^seq(-4, 0, length = 50)
)
colSums(is.na(df.us))
logit_fit <- train(
loan_default~ .,
data = df.us,
method = "glmnet",
trControl = ctrl,
tuneGrid =tuneGrid,
metric = "ROC",
family = "binomial"
)
plot(logit_fit)
logit_fit$bestTune
logit_fit$results
coef(logit_fit$finalModel,logit_fit$bestTune$lambda)
pred.test <- holdout %>% dplyr::select(loan_default)
pred.test$netprob <- predict(logit_fit,
newdata = holdout,
type = "prob")[,"Yes"]
pred.test$netclass<- predict(logit_fit,
newdata = holdout,
type="raw")
table(pred.test$loan_default)
table(pred.test$netclass)
confusionMatrix(pred.test$netclass, pred.test$loan_default,positive="Yes")
pred <- prediction(pred.test$netprob, pred.test$loan_default)
perf <- performance(pred, "tpr", "fpr")
# Plot Curve
plot(perf, col = "blue", lwd = 2, main = "ROC Curve for Model 1")
abline(a = 0, b = 1, col = "red", lty = 2, lwd = 2)
auc <- performance(pred, "auc")@y.values[[1]]
print(paste("The AUC is ",round(auc,2)))
pred <- ROCR::prediction(pred.test$netprob,holdout$loan_default)
prec <- ROCR::performance(pred, "prec")
rec <- ROCR::performance(pred, "rec")
precision <- prec@y.values[[1]]
recall <- rec@y.values[[1]]
f1=2*precision*recall/(precision+recall)
f1[is.nan(f1)]=0
cutoffs=prec@x.values[[1]]
df_f1=data.frame(f1,cutoffs)
opt_idx <- which.max(f1)
opt_f1 <- df_f1[opt_idx,]
opt_f1
pred <- ROCR::prediction(pred.test$netprob,holdout$loan_default)
perf=performance(pred,"tpr","fpr")
tpr=perf@y.values[[1]]
fpr=perf@x.values[[1]]
thr=perf@alpha.values[[1]]
j=tpr-fpr
best_j=thr[which.max(j)]
print(best_j)
pred.test$netclass_51 <- factor(ifelse(pred.test$netprob > 0.521216,
"Yes","No"),
levels=c("Yes","No"))
table(pred.test$loan_default)
table(pred.test$netclass_51)
confusionMatrix(pred.test$netclass_51, pred.test$loan_default,positive="Yes")
pred.test$netclass_46 <- factor(ifelse(pred.test$netprob > 0.4826321,
"Yes","No"),
levels=c("Yes","No"))
table(pred.test$loan_default)
table(pred.test$netclass_46)
confusionMatrix(pred.test$netclass_46, pred.test$loan_default,positive="Yes")
saveRDS(logit_fit,"dec3_logit_model.rds")
saveRDS(pred.test,"dec_pred_test.rds")
coef(logit_fit$finalModel,logit_fit$bestTune$lambda)
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)
if(require(pacman)==0)
{install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes)
if (!require(mlba)) {
library(devtools)
install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
df = read.csv("score.csv", stringsAsFactors = TRUE)
train_df = read.csv("train.csv", stringsAsFactors = TRUE)
train_median <- median(train_df$revol_util, na.rm = TRUE)
str(df)
#Split data into categorical and numeric datasets
bank_numeric <- select_if(df, is.numeric)
bank_factor <- select_if(df, is.factor)
#Ensure dataframes correctly split
head(bank_numeric)
head(bank_factor)
#Summary Statistics for numeric Variables
summary(bank_numeric)
plot_missing(df)
# Look at structure and ensure these are all factor variables
str(bank_factor)
# Histograms for numeric variables
DataExplorer::plot_histogram(bank_numeric,
ggtheme=theme_minimal(),
ncol=3,
nrow=3)
# Boxplots based off the loan defaulting or not
#bank_numeric$loan_default <- df$loan_default
#DataExplorer::plot_boxplot(bank_numeric, by="loan_default",
#ggtheme = theme_minimal(),
#ncol=3,
#nrow=3)
# Create a heat map to check correlation between variables
DataExplorer::plot_correlation(bank_numeric,
type="continuous",
cor_args = list("use" = "pairwise.complete.obs")
)
# Remove these variables
df<-df%>%
select(-installment, -acc_now_delinq, -mths_since_last_delinq, -delinq_2yrs)
dim(df)
DataExplorer::plot_bar(bank_factor,
ggtheme = theme_minimal(),
ncol=4,
nrow=5)
# Remove variables that are not seen as needed
df<-df%>%
select(-emp_title, -issue_d, -title, -application_type, -hardship_flag, -earliest_cr_line, -last_credit_pull_d, -address, -sub_grade)
dim(df)
#Checking missing Values
colSums(is.na(df))
#DEC
df$fico_score <- (df$fico_range_low + df$fico_range_high) / 2
df <- df %>% select(-fico_range_low, -fico_range_high)
#cap <- quantile(df$annual_inc, 0.99)
#df$annual_inc[df$annual_inc > cap] <- cap
# Median imputation on the mort_acc variable
df$mort_acc[is.na(df$mort_acc)] <- 1
#df$mort_acc[is.na(df$mort_acc)] <- median(df$mort_acc, na.rm = TRUE)
sum(is.na(df$mort_acc))
#Median imputation to the pub_rec_bankrupcies variable
df$pub_rec_bankruptcies[is.na(df$pub_rec_bankruptcies)] <- 0
sum(is.na(df$pub_rec_bankruptcies))
# Removing rows where revol_util is NA
#df <- df[!is.na(df$revol_util), ]
df$revol_util[is.na(df$revol_util)] <- train_median
#nrow(df)
# Capping the revol_util variable
Q1 <- quantile(df$revol_util, 0.25, na.rm = TRUE)
Q3 <- quantile(df$revol_util, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$revol_util[df$revol_util > upper_bound] <- upper_bound
summary(df$revol_util)
boxplot(df$revol_util, main = "Revolving Utilization", horizontal = TRUE)
# Capping the dti outlier
Q1 <- quantile(df$dti, 0.25, na.rm = TRUE)
Q3 <- quantile(df$dti, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$dti[df$dti > upper_bound] <- upper_bound
boxplot(df$dti, main = "DTI", horizontal = TRUE)
#Removing the "other" and "none" values
#df <- df[!df$home_ownership %in% c("OTHER", "NONE"), ]
df$home_ownership[df$home_ownership %in% c("OTHER","NONE")] <- "RENT"
df$home_ownership <- droplevels(df$home_ownership)
table(df$home_ownership)
#Reduce dimensions in the purpose variable
df$purpose <- fct_other(df$purpose, keep = c("debt_consolidation", "credit_card", "home_improvement"), other_level = "other")
table(df$purpose)
# Reducing dimensions on emp_length
df <- df %>%
mutate(emp_length = case_when(
emp_length %in% c("< 1 year", "1 year", "2 years", "3 years") ~ "0-3 years",
emp_length %in% c("4 years", "5 years", "6 years") ~ "4-6 years",
emp_length %in% c("7 years", "8 years", "9 years") ~ "7-9 years",
emp_length == "10+ years" ~ "10+ years",
TRUE ~ "Unknown"
))
df$emp_length <- factor(df$emp_length,
levels = c("0-3 years", "4-6 years", "7-9 years", "10+ years","Unknown"),
ordered = TRUE)
#df <- df %>%
#  filter(!is.na(emp_length) & emp_length != "Unknown")
# Dummy encoding all remaining categorical variables
df <- fastDummies::dummy_cols(df,
select_columns = c(
"term",
"home_ownership",
"verification_status",
"grade",
"purpose",
"initial_list_status",
"debt_settlement_flag",
"emp_length"),
remove_first_dummy = TRUE,      # avoids dummy variable trap
remove_selected_columns = TRUE) # removes original factor columns
# dti * int_rate interaction term
df$interaction_dti_interest <- df$dti * df$int_rate
# Show correlation
hist(df$interaction_dti_interest,
main = "Interaction",
xlab = "DTI * int_rate",
col = "lightblue",
border = "white")
summary(df)
names(df) <- gsub(" ", "_", names(df))
names(df) <- gsub("-", "_", names(df))
names(df) <- gsub("\\+", "plus", names(df))
saveRDS(df, "score_df.rds")
xgb_model = readRDS("dec3_logit_model.rds")
# Predict using the trained model
#y_pred <- predict(xgb_Model, newdata=df)
#y_pred <- ifelse(y_pred == "Yes", 1, 0)
pred.prob <- predict(xgb_model,
newdata = df,
type = "prob")[,"Yes"]
y_pred <- factor(ifelse(pred.prob > 0.5288394,
"1","0"),
levels=c("1","0"))
# Combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)
# Write to CSV
write.csv(predictions, "group5AA_Black-Boopathy_submission_file.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)
if(require(pacman)==0)
{install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes)
if (!require(mlba)) {
library(devtools)
install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
df = read.csv("score.csv", stringsAsFactors = TRUE)
train_df = read.csv("train.csv", stringsAsFactors = TRUE)
train_median <- median(train_df$revol_util, na.rm = TRUE)
str(df)
#Split data into categorical and numeric datasets
bank_numeric <- select_if(df, is.numeric)
bank_factor <- select_if(df, is.factor)
#Ensure dataframes correctly split
head(bank_numeric)
head(bank_factor)
#Summary Statistics for numeric Variables
summary(bank_numeric)
plot_missing(df)
# Look at structure and ensure these are all factor variables
str(bank_factor)
# Histograms for numeric variables
DataExplorer::plot_histogram(bank_numeric,
ggtheme=theme_minimal(),
ncol=3,
nrow=3)
# Boxplots based off the loan defaulting or not
#bank_numeric$loan_default <- df$loan_default
#DataExplorer::plot_boxplot(bank_numeric, by="loan_default",
#ggtheme = theme_minimal(),
#ncol=3,
#nrow=3)
# Create a heat map to check correlation between variables
DataExplorer::plot_correlation(bank_numeric,
type="continuous",
cor_args = list("use" = "pairwise.complete.obs")
)
# Remove these variables
df<-df%>%
select(-installment, -acc_now_delinq, -mths_since_last_delinq, -delinq_2yrs)
dim(df)
DataExplorer::plot_bar(bank_factor,
ggtheme = theme_minimal(),
ncol=4,
nrow=5)
# Remove variables that are not seen as needed
df<-df%>%
select(-emp_title, -issue_d, -title, -application_type, -hardship_flag, -earliest_cr_line, -last_credit_pull_d, -address, -sub_grade)
dim(df)
#Checking missing Values
colSums(is.na(df))
#DEC
df$fico_score <- (df$fico_range_low + df$fico_range_high) / 2
df <- df %>% select(-fico_range_low, -fico_range_high)
#cap <- quantile(df$annual_inc, 0.99)
#df$annual_inc[df$annual_inc > cap] <- cap
# Median imputation on the mort_acc variable
df$mort_acc[is.na(df$mort_acc)] <- 1
#df$mort_acc[is.na(df$mort_acc)] <- median(df$mort_acc, na.rm = TRUE)
sum(is.na(df$mort_acc))
#Median imputation to the pub_rec_bankrupcies variable
df$pub_rec_bankruptcies[is.na(df$pub_rec_bankruptcies)] <- 0
sum(is.na(df$pub_rec_bankruptcies))
# Removing rows where revol_util is NA
#df <- df[!is.na(df$revol_util), ]
df$revol_util[is.na(df$revol_util)] <- train_median
#nrow(df)
# Capping the revol_util variable
Q1 <- quantile(df$revol_util, 0.25, na.rm = TRUE)
Q3 <- quantile(df$revol_util, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$revol_util[df$revol_util > upper_bound] <- upper_bound
summary(df$revol_util)
boxplot(df$revol_util, main = "Revolving Utilization", horizontal = TRUE)
# Capping the dti outlier
Q1 <- quantile(df$dti, 0.25, na.rm = TRUE)
Q3 <- quantile(df$dti, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$dti[df$dti > upper_bound] <- upper_bound
boxplot(df$dti, main = "DTI", horizontal = TRUE)
#Removing the "other" and "none" values
#df <- df[!df$home_ownership %in% c("OTHER", "NONE"), ]
df$home_ownership[df$home_ownership %in% c("OTHER","NONE")] <- "RENT"
df$home_ownership <- droplevels(df$home_ownership)
table(df$home_ownership)
#Reduce dimensions in the purpose variable
df$purpose <- fct_other(df$purpose, keep = c("debt_consolidation", "credit_card", "home_improvement"), other_level = "other")
table(df$purpose)
# Reducing dimensions on emp_length
df <- df %>%
mutate(emp_length = case_when(
emp_length %in% c("< 1 year", "1 year", "2 years", "3 years") ~ "0-3 years",
emp_length %in% c("4 years", "5 years", "6 years") ~ "4-6 years",
emp_length %in% c("7 years", "8 years", "9 years") ~ "7-9 years",
emp_length == "10+ years" ~ "10+ years",
TRUE ~ "Unknown"
))
df$emp_length <- factor(df$emp_length,
levels = c("0-3 years", "4-6 years", "7-9 years", "10+ years","Unknown"),
ordered = TRUE)
#df <- df %>%
#  filter(!is.na(emp_length) & emp_length != "Unknown")
# Dummy encoding all remaining categorical variables
df <- fastDummies::dummy_cols(df,
select_columns = c(
"term",
"home_ownership",
"verification_status",
"grade",
"purpose",
"initial_list_status",
"debt_settlement_flag",
"emp_length"),
remove_first_dummy = TRUE,      # avoids dummy variable trap
remove_selected_columns = TRUE) # removes original factor columns
# dti * int_rate interaction term
df$interaction_dti_interest <- df$dti * df$int_rate
# Show correlation
hist(df$interaction_dti_interest,
main = "Interaction",
xlab = "DTI * int_rate",
col = "lightblue",
border = "white")
summary(df)
names(df) <- gsub(" ", "_", names(df))
names(df) <- gsub("-", "_", names(df))
names(df) <- gsub("\\+", "plus", names(df))
saveRDS(df, "score_df.rds")
xgb_model = readRDS("dec3_logit_model.rds")
# Predict using the trained model
#y_pred <- predict(xgb_Model, newdata=df)
#y_pred <- ifelse(y_pred == "Yes", 1, 0)
pred.prob <- predict(xgb_model,
newdata = df,
type = "prob")[,"Yes"]
y_pred <- factor(ifelse(pred.prob > 0.5266154,
"1","0"),
levels=c("1","0"))
# Combine predictions with IDs
predictions <- data.frame(ID = df$ID, loan_status = y_pred)
# Write to CSV
write.csv(predictions, "group5AA_Black-Boopathy_submission_file.csv", row.names = FALSE)
