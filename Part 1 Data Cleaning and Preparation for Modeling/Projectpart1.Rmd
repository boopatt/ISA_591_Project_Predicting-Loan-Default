---
title: 'Project Part1: EDA'
author: "Aaron Black, Thenmozhi Boopathy"
date: "2025-09-23"
output:
  #pdf_document:
  #  toc: true
  html_document:
    code_folding: show
    df_print: paged
    theme: cerulean
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)

if(require(pacman)==0)
   {install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes,dslabs,scatterplot3d)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
```
## Introduction

This HTML file contains the data cleaning methods we have utilized to better predict whether someone will default on their loan or not based on a data set containing 237730 observations.

## 1. Data Overview and Summary Statistics

First, we are going to load the dataset into our RMarkdown file and get some basic information about our variables. This loaded data will be cleaned to help us eventually use the model for our holdout data. Our target variable is loan_default, meaning whether or not someone will default on a loan. Below is our starting code for the analysis.

```{r cars}
#Read in the data 
df = read.csv("train.csv", stringsAsFactors = TRUE)
str(df)
```
Notice how there are 18 character variables and 18 numeric variables. We will now split these data structures up to get more specific visuals with both structures. 

```{r pressure, echo=FALSE}
#Split data into categorical and numeric datasets 
bank_numeric <- select_if(df, is.numeric)
bank_factor <- select_if(df, is.factor)
#Ensure dataframes correctly split
head(bank_numeric)
head(bank_factor)
```

The data successfully split. Now, we can utilize different visualization methods to identify various anomalies, outliers, missing values, and where we can change the variable type to make our analysis the best it can be. Let's look at some preliminary observations. 

```{r}
#Summary Statistics for numeric Variables 
summary(bank_numeric)
```
Each variable gives important values such as the mean and median with the min and max values helping identify outliers. We can also find early indicators of numeric variables that could potentially be manipulated into factor variables or factor lumped to help our prediction model. 

Let's remember to note the missing values: 

```{r}
plot_missing(df)
```

- revol_util = 177 missing values 
- mort_acc = 22854 missing values 
- pub__rec_bankruptcies = 336 missing values 
- mths_since_last_delinq = 126517 missing

```{r}
# Look at structure and ensure these are all factor variables
str(bank_factor)

```
As we can see, we have successfully identified our factor variables. Making a table wouldn't be useful in this case because certain variables have a significant amount of levels such as address, title, and emp_tile.  Please note that we will make changes to the data in our main dataframe, we are just using the split datasets to look deeper into the data. 

## 2. Data visualization

Now, we can dig further into our data and identify what we need to do with each specific variable based on our output. It is important to figure out whether to remove, alter, or keep each specific variables so we can have the best data to eventually train a champion model.  


```{r}
# Histograms for numeric variables

DataExplorer::plot_histogram(bank_numeric,
                             ggtheme=theme_minimal(),
                             ncol=3,
                             nrow=3)

```

These variables can easily be seen for their skew, symmetry, and range. If the range is large from the models, like pub_rec, revol_bal, dti, and annual_inc, this indicates outliers in the data. 

Lets look at the box plots for the numeric variables to confirm our suspicions on outliers and see if there are important variables to keep in our dataset based on loan_default.


```{r}
# Boxplots based off the loan defaulting or not 
bank_numeric$loan_default <- df$loan_default
DataExplorer::plot_boxplot(bank_numeric, by="loan_default",
                           ggtheme = theme_minimal(),
                          ncol=3,
                          nrow=3)
```

Based on these results, we can confirm outliers in the data and deal with them accordingly. Also, we see that these numeric variables can have correlation to one defaulting on a loan. Lets look at a heat map to finalize some of the cleaning techniques we will perform. 

```{r}
# Create a heat map to check correlation between variables
DataExplorer::plot_correlation(bank_numeric,
                               type="continuous",
                               cor_args = list("use" = "pairwise.complete.obs")
                               )

```
Total_acc and open_acc has correlated metrics with an R^2 of .68, while pub_rec and pub_rec_bankrupcies have correlated features with an R^2 of 0.68. Installment has an extremely high correlation to loan amount, showing we can remove this variable.

After using some visualization techniques to look at our numeric variables, we have listed below our cleaning strategys for each one. 

### Keep the Same

These 7 variables below have key indicators that will help us predict loan_default based on the summary statistics, boxplot, heatmap, and bar plot visualizations. 

- loan_amnt 
- int_rate 
- annual_inc  
- open_acc  
- total_acc  
- fico_range_low
- fico_range_high 
- inq_last_6mnths

### Change and Recode

These numeric variables were flagged as having anomalies, outliers, or ranges that could be manipulated to better suit our model. 

- dti: There is one outlier. We can figure out how to remove this extreme value.
- pub_rec: One outlier that we must examine. 
- revol_bal: One outlier that we must examine.
- revol_util: Contains a massive outlier and missing values, we must analyze further.
- mort_acc: Many null values. We will analyze more in depth in the upcoming sections.  
- pub_rec_bankruptcies:  Null values will be further analyzed. 
 

### Variables we are Removing

- acc_now_delinq: We are removing this variable because most of the values are zero and it wouldn't give us any special indication of loan default.
- mths_since_last_delinq: We will remove this variable because there are too many null values(53.22%).Most people probably don't have any delinquency, so this variable can be disregarded.
- delinq_2yrs: Most of the values are zero.  but we can also just get rid of it.
- installment: We are removing this variable because of its high correlation (0.95) to loan amount, meaning they provide the same information. 

```{r}
# Remove these variables
df<-df%>%
  select(-installment, -acc_now_delinq, -mths_since_last_delinq, -delinq_2yrs)
dim(df)
```
We now have only 32 variables in our original data frame. (36-4=32). None of these variables show major distribution between the 'yes' and 'no decision of a loan default based on the boxplot and amount of null values. 

### Cateogrical Variables: 

Now, we will look at the factor variables and decide where to make changes based on the prediction power and levels. We will also recode and lump variables together if there are too many levels to the data or remove when there's too many levels in itself.
```{r}
# Look at structure and levels for categorical variables
DataExplorer::plot_bar(bank_factor,
                       ggtheme = theme_minimal(),
                       ncol=4,
                       nrow=5)



```
There are 12 variables shown here. Notice how some variables are not included in the visual, such as emp_title, issue_d, earliest_cr_line, address, and last_credit_pull_d. These variables all have a significant amount of levels, meaning they could either be manipulated to a numeric variable or be removed because there would way too many dummy variables. Even some of the bar plots have too many levels that will need to be analyzed further later in our analysis. 

Now, we will identify what we are doing with each variable. For the categorical variables we are keeping, we will specify changing the leels or leaving the data as it is. 

#### Keep the Same

- Term: 2 levels, we will keep the same 
- Grade: 7 levels, keep the same
- Verification_status: 2 levels, Keep the same
- Initial_list_status: 2 levels, Keep the same
- Debt_settlement_flag: a "yes" decision has a high correlation to "yes" to loan_default, keep the same

#### Change and Recode
- Emp_length: Use bins to cluster years together
- Home_ownership: Remove the “Other” and “None” variable
- Purpose: Keep “Debt consolidation,” cluster the rest into “Other”

#### Variables we are Removing
- Emp_title: Remove (too many levels)
- Issue_d: Remove (No significance to loan_default_
- Title: Remove (Too many levels)
- Application_type: Remove (Vast majority of values in "individual" value)
- Hardship_flag: Remove (only 1 level)
- Earliest_cr_line: Dates do not seem significant in predicting loan default
- Last_credit_pull_d: Dates do not seem significant in predicting loan default
- Address (zip code): Discriminatory variable, not significant in explaining loan default
- sub_grade: Has the same properties as grade, not needed in our model
```{r}
# Remove variables that are not seen as needed
df<-df%>%
  select(-emp_title, -issue_d, -title, -application_type, -hardship_flag, -earliest_cr_line, -last_credit_pull_d, -address, -sub_grade)
dim(df)
```
We now have 23 Variables in our model (32-8=23)

## 3. Handling Missing Data 

Earlier, we identified some of the missing values in different variables. Although we removed the mnths_since_last_delinq variable, we still have three variables that are missing data: 

```{r}
#Checking missing Values
colSums(is.na(df))
```

### 1. mort_acc
There are 22854 missing values in the # of mortgage accounts row. In this case, we can assume that anyone who didn't input a value in this column does not want to disclose that they have a mortgage account. Therefore, we can impute these missing valuee. With the median being 1 and the mean being 1.81, we can perform median imputation on this variable.

```{r}
# Median imputation on the mort_acc variable
df$mort_acc[is.na(df$mort_acc)] <- 1
sum(is.na(df$mort_acc))
```

### 2. pub_rec_bankrupcies

With 336 missing values missing with this variable, we can either impute values of get rid of these rows of data. The vast majority of these values are 0, so lets perform median imputation and plug in 0 to these values. 
```{r}
#Median imputation to the pub_rec_bankrupcies variable
df$pub_rec_bankruptcies[is.na(df$pub_rec_bankruptcies)] <- 0
sum(is.na(df$pub_rec_bankruptcies))

```
### 3. revol_util
There are 177 missing values with this variables, indiciating either removing the rows with these missing values or imputing a value that won't vastly change the variable integrity. Since the summary statistics show the revolving utility to have a larger range, imputing data in these variables could slighty skew the data into a misleading direction. Therefore, it is best to delete these 177 rows of data. 
```{r}
# Removing rows where revol_util is NA 
df <- df[!is.na(df$revol_util), ]
nrow(df)
```
237730-237553 = 177 rows deleted from the data. 

### 4. emp_length


```{r}
DataExplorer::plot_bar(df$emp_length,
                       ggtheme = theme_minimal(),
                       ncol=4,
                       nrow=5)

table(df$emp_length)


emp_numeric <- df$emp_length

emp_numeric <- gsub("< 1 year", "0", emp_numeric)
emp_numeric <- gsub("10\\+ years", "10", emp_numeric)
emp_numeric <- gsub(" years", "", emp_numeric)
emp_numeric <- gsub(" year", "", emp_numeric)
emp_numeric[emp_numeric == ""] <- NA
emp_numeric <- as.numeric(emp_numeric)
summary(emp_numeric)

emp_numeric[is.na(emp_numeric)] <- median(emp_numeric, na.rm = TRUE)

df$emp_length <- emp_numeric

table(df$emp_length)
table(bank_factor$emp_length)
```

## 4. Outlier/Anomaly Detection
Throughout our data cleaning process, we have identified numerous variables with outliers. Below, we will decide what to do with these variables. In deciding what to do, we must ask ourselves: Are these outliers realistic in context of the real world? 

### 1. pub_rec

This variable has 1 extreme value at 86 when most values are 0. At that point of having that many public derogatory records, you are bound to default on your loan. Therefore, this value can remain in the data set. 

### 2. revol_util 

This singular outlier is extreme given the calculation for this value is credit used / total credit. This would mean that this individual would use an absurd amount of credit used. We will cap this variable to keep the integrity of the data realistic.
```{r}
# Capping the revol_util variable
Q1 <- quantile(df$revol_util, 0.25, na.rm = TRUE)
Q3 <- quantile(df$revol_util, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
upper_bound <- Q3 + 1.5 * IQR
df$revol_util[df$revol_util > upper_bound] <- upper_bound
summary(df$revol_util)
boxplot(df$revol_util, main = "Revolving Utilization", horizontal = TRUE)
```

### 3. dti

There is 1 extreme outlier that should be referred to as an anomaly. The next highest value would be under 50, and having a debt-to-income ratio that high is unheard of. Let's cap this outlier. 

```{r}
# Capping the dti outlier
Q1 <- quantile(df$dti, 0.25, na.rm = TRUE)
Q3 <- quantile(df$dti, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

upper_bound <- Q3 + 1.5 * IQR
df$dti[df$dti > upper_bound] <- upper_bound
boxplot(df$dti, main = "DTI", horizontal = TRUE)
```


### 4. annual_inc

There is one outlier that significantly larger than the rest. However, we will be keeping this variable because it is realistic for someone to make that much money annually. Think C-suite executives or professional athletes. 

### 5. total_acc

There is one significant outlier, but the skew of the data is more intriguing than that outlier. We will keep this outlier. 

### 6. open_acc

Two specific outliers in the data, we do not see any need to remove these outliers given it is one value of hundreds of thousands and there's one value that reflect a "yes" or "no" response in loan_default. 

### 7. revol_bal 

This outlier is realistic in the same sense that the annual income outlier is realistic. Some people have a significant amount of money and utilize these funds to start businesses. 

## 5. Data Transformation 

When deciding to transform certain variables in our data, we need to ensure there is a wide enough skew. Many variables, such as fico_range_high and fico_range_low have very obvious right skews in their data, driving up the mean and keeping the median in a lower value. Although this could be transformed, we do not want to alter the data to lose its integrity and predictability within our future training model. 

## 6. Dimension Reduction

Dimension reduction could be an important part of training an accurate model, especially with variables that have an extreme amount of levels. Let's take subgrade for example. We could use the Principal Component Analysis Method (PCA) to capture the most important information on that variable while reducing the 35 levels in that variable alone. However, we argue that changing this variable to numeric can better capture its predicting power. Overall, performing the PCA method does not seem like our predictive power would increase for any categorical variable with multiple levels. 

### Feature Selection
For feature selection, we will be removing the "other" and "none" sections in the home_ownership variable. 
```{r}
#Removing the "other" and "none" values 
df <- df[!df$home_ownership %in% c("OTHER", "NONE"), ]
df$home_ownership <- droplevels(df$home_ownership)
table(df$home_ownership)
```
With this, we removed the levels "Other" and "None" from this data set, giving the model a clearer set of three levels that have a good amount of values. 

### Reducing Dimensionality

The purpose variable shows many levels that have few values assigned to those levels. There are three main categories for this variable, so we will reduce the dimension and assign the rest of the values to the "other" dimension. 
```{r}
#Reduce dimensions in the purpose variable
df$purpose <- fct_other(df$purpose, keep = c("debt_consolidation", "credit_card", "home_improvement"), other_level = "other")
table(df$purpose)
```
We also want to put emp_length into 5 main groups so we can dummy encode this variable later. We will put these dimension in 5 levels: 0-3 years, 4-6 years, 7-9 years, and 10+ years 
```{r}
# Reducing dimensions on emp_length
df <- df %>%
  mutate(emp_length = case_when(
    emp_length %in% c("0","1","2", "3") ~ "0-3 years",
    emp_length %in% c("4", "5", "6") ~ "4-6 years",
    emp_length %in% c("7", "8", "9") ~ "7-9 years",
    emp_length == "10" ~ "10+ years"
  ))
df$emp_length <- factor(df$emp_length,
                                levels = c("0-3 years", "4-6 years", "7-9 years", "10+ years"),
                                ordered = TRUE)
#df <- df %>%
#  filter(!is.na(emp_length) & emp_length != "Unknown")
```

Overall, uninformative variables have been dropped and factors have been clustered. We can now focus on our current dimensions that have been kept. 

### 7. Encoding Categorical Variables 

Encoding these categorical variables are important for our logistic regression. Without dummy-encoding, the models won't be able to provide any significant predictors. Below, we will be using the fastdummies package to make our model useful for a logistic (glm) regression. 
```{r}
# Dummy encoding all remaining categorical variables 
df <- fastDummies::dummy_cols(df,
                              select_columns = c("loan_default",
                                                 "term",
                                                 "home_ownership",
                                                 "verification_status",
                                                 "grade",
                                                 "purpose",
                                                 "initial_list_status",
                                                 "debt_settlement_flag",
                                                 "emp_length"),
                              remove_first_dummy = TRUE,      # avoids dummy variable trap
                              remove_selected_columns = TRUE) # removes original factor columns
```

All of our factor variables are now dummy-encoded and ready to be train in a logistic regression model. We decided to dummy-encode all of these variables to be consistent across the board in our analysis. 

## 8. Feature Creation

There are many different feature creation methods that we could perform to potentially make our model the best in predicting loan default. That being said, these methods could hurt us as much as benefit our predicition accuracy if we perform something confusing or misleading. 

For our feature creation method of choice, we found that the debt to income ratio and the interest rate together could indicate a higher probability to loan default. Therefore, we tried pefroming an interaction term: 

```{r}
# dti * int_rate interaction term 
df$interaction_dti_interest <- df$dti * df$int_rate
# Show correlation
hist(df$interaction_dti_interest,
     main = "Interaction",
     xlab = "DTI * int_rate",
     col = "lightblue",
     border = "white")

```

There is a pretty significant right skew in this interaction term. For now, we will utilize this interaction term to help train our model in the upcoming sections. 

## Conclusion: Summary of our Cleaned Data

After performing all of these steps to create the best data frame to train a model, we have resulted in this dataset: 

```{r}
summary(df)
```
This summary shows the finalized data set we will be using to train our model. We will now convert our data into an RDS file and continue building out our model. While there could be other actions performed, this will be a great start on our path to have the champion model! 

```{r}
# Convert into an rds file
saveRDS(df, file = "group5AA_Black-Boopathy_train.rds")
```

